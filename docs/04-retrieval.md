# 5. Fase 2: RETRIEVAL (BÃºsqueda)

Cuando el usuario hace una pregunta, se ejecuta un pipeline de **7+ pasos** antes de generar la respuesta.

---

## Paso 2.1: Semantic Router (router.py)

```python
# Â¿QuÃ© hace?
# Clasifica la INTENCIÃ“N del usuario ANTES de buscar
# Es un guardrail (barrera de seguridad)

class SemanticRouter:
    def route(self, query: str) -> str:
        # Usa el LLM para clasificar en 3 categorÃ­as:
        # "SEARCH"   â†’ Pregunta sobre el documento
        # "GREETING" â†’ Saludo ("hola", "buenos dÃ­as")
        # "OFF_TOPIC"â†’ Fuera de tema ("cuÃ©ntame un chiste")
        ...
```

### Â¿Por quÃ© un Router?

```text
Sin Router:
  User: "hola"
  â†’ Sistema busca "hola" en ChromaDB
  â†’ Encuentra chunks aleatorios con baja similitud
  â†’ LLM genera respuesta confusa basada en contexto irrelevante
  â†’ Gastamos 4 LLM calls + tiempo en bÃºsqueda inÃºtil

Con Router:
  User: "hola"
  â†’ Router detecta: GREETING
  â†’ Responde directamente: "Â¡Hola! Â¿En quÃ© puedo ayudarte?"
  â†’ 1 LLM call, 0 bÃºsquedas

  User: "tu madre"
  â†’ Router detecta: OFF_TOPIC
  â†’ Responde: "Solo respondo sobre el documento."
  â†’ El sistema no pierde tiempo ni recursos
```

---

## Paso 2.2: Query Normalization (pipeline.py)

```python
def _normalize_query(self, query: str) -> str:
    """
    "Â¿DÃ³nde se habla de LIME?"
    â†’ "donde se habla de lime"
    """
    q = query.lower()

    # Quitar acentos: "Ã³" â†’ "o"
    q = unicodedata.normalize("NFD", q)
    q = "".join(c for c in q if unicodedata.category(c) != "Mn")

    # Quitar puntuaciÃ³n: "Â¿?!Â¡" â†’ ""
    q = re.sub(r"[Â¿?!Â¡.,;:\-\"']", "", q)

    # Normalizar espacios
    q = " ".join(q.split())
```

### Â¿Por quÃ© normalizar?

```text
Sin normalizaciÃ³n, estas 3 queries son DIFERENTES para el cache:
  "Â¿DÃ³nde se habla de LIME?"
  "donde se habla de lime"
  "Donde se habla de LIME???"

Con normalizaciÃ³n, las 3 se convierten en:
  "donde se habla de lime"
  â†’ Cache hit en la segunda y tercera pregunta
  â†’ Ahorramos 2 llamadas al LLM
```

---

## Paso 2.3: Semantic Cache (cache.py)

Este es uno de los componentes mÃ¡s sofisticados. Funciona en dos niveles:

### NIVEL 1: NormalizaciÃ³n de intenciÃ³n (LLM barato)

```python
# Reduce la query a su forma canÃ³nica

def _get_canonical_intent(self, query: str) -> str:
    # "donde aparece lime"     â†’ "ubicacion lime"
    # "donde se define lime"   â†’ "ubicacion lime"
    # "en que parte esta lime" â†’ "ubicacion lime"
    ...
```

### NIVEL 2: Similitud por embeddings

```python
# Compara la intenciÃ³n canÃ³nica contra todas las cacheadas

def get(self, query: str) -> Optional[Tuple[str, List[Document]]]:
    canonical = self._get_canonical_intent(query)
    query_vector = self.embeddings.embed_query(canonical)

    # Calcular similitud contra TODOS los vectores cacheados
    # (usando Ã¡lgebra lineal, muy rÃ¡pido)
    similarities = self._vectors @ query_norm

    best_score = max(similarities)
    if best_score >= 0.95:  # 95% similar
        return cached_response  # Â¡Cache HIT!
```

### Â¿Por quÃ© dos niveles?

```text
Solo con embeddings (Nivel 2):
  "donde aparece lime" â†’ embedding A
  "donde se define lime" â†’ embedding B
  similitud(A, B) = 0.80 â†’ MISS (debajo del threshold 0.95)
  â†’ Ejecutamos TODO el pipeline de nuevo ğŸ˜

Con normalizaciÃ³n de intenciÃ³n (Nivel 1 + 2):
  "donde aparece lime"   â†’ LLM â†’ "ubicacion lime" â†’ embedding X
  "donde se define lime"  â†’ LLM â†’ "ubicacion lime" â†’ embedding Y
  similitud(X, Y) = 1.00 â†’ HIT âœ…
  â†’ Retornamos respuesta cacheada en milisegundos ğŸš€
```

### Eviction Strategy (LFU con decay temporal)

```python
def _evict_least_used(self):
    # Cuando el cache estÃ¡ lleno (500 entradas), eliminamos la peor
    # Score = veces_usado / horas_de_vida

    # Ejemplo:
    # Entry A: usada 10 veces, creada hace 2 horas â†’ score = 5.0
    # Entry B: usada 1 vez, creada hace 24 horas   â†’ score = 0.04
    # â†’ Eliminamos Entry B (poco usada Y vieja)
    ...
```

---

## Paso 2.4: Query Contextualization (contextualizer.py)

```python
# Â¿QuÃ© hace?
# Resuelve pronombres y referencias ambiguas usando el historial

class QueryContextualizer:
    def contextualize(self, query: str, history: list) -> str:
        # Historial: El usuario preguntÃ³ sobre LIME

        # query = "Â¿En quÃ© pÃ¡gina estÃ¡?"
        # â†’ "Â¿En quÃ© pÃ¡gina estÃ¡ LIME?"

        # query = "Â¿Y eso quÃ© ventajas tiene?"
        # â†’ "Â¿QuÃ© ventajas tiene LIME?"
        ...
```

### Â¿Por quÃ© es necesario?

```text
ConversaciÃ³n:
  User: "Â¿QuÃ© es LIME?"
  Bot: "LIME es una tÃ©cnica de explicabilidad..."
  User: "Â¿En quÃ© pÃ¡gina estÃ¡?"           â† Â¿QUÃ‰ estÃ¡?

  Sin contextualizaciÃ³n:
    Busca "en quÃ© pÃ¡gina estÃ¡" en ChromaDB
    â†’ No encuentra nada relevante (la query no tiene sustancia)

  Con contextualizaciÃ³n:
    "Â¿En quÃ© pÃ¡gina estÃ¡?" â†’ "Â¿En quÃ© pÃ¡gina estÃ¡ LIME?"
    Busca "en quÃ© pÃ¡gina estÃ¡ LIME" en ChromaDB
    â†’ Encuentra chunks relevantes sobre LIME
```

### Guard de seguridad

```python
# El contextualizer puede "expandir de mÃ¡s"
# Guard: rechazar si la salida es 3x mÃ¡s larga que la entrada

if len(new_query) > len(query) * 3:
    return query  # Fallback a la original

# Ejemplo que se rechaza:
# Input:  "lime" (4 chars)
# Output: "Â¿QuÃ© es la tÃ©cnica LIME y cÃ³mo funciona en el contexto
#          de explicabilidad de modelos de machine learning?" (90 chars)
# 90 > 4*3=12 â†’ RECHAZADO, se usa "lime" directamente
```

---

## Paso 2.5: HyDE â€” Hypothetical Document Embeddings (hyde.py)

```python
# Â¿QuÃ© hace?
# Genera un "documento ficticio" que RESPONDERÃA la pregunta
# y lo usa para buscar documentos REALES similares

class HyDEGenerator:
    def generate(self, query: str) -> str:
        # Input: "Â¿QuÃ© es LIME?"
        # Output: "LIME (Local Interpretable Model-agnostic Explanations)
        #          es una tÃ©cnica de explicabilidad que genera
        #          perturbaciones locales para aproximar el comportamiento
        #          de un modelo complejo con un modelo interpretable..."
        ...
```

### Â¿Por quÃ© funciona mejor que buscar directamente?

```text
PROBLEMA: AsimetrÃ­a query-documento

  La QUERY del usuario:    "Â¿QuÃ© es LIME?"  (pregunta corta)
  El DOCUMENTO real dice:  "LIME (Local Interpretable Model-agnostic
                           Explanations) es una tÃ©cnica..." (texto largo)

  Embedding de la query:     [0.1, 0.2, ...]  â†’ Espacio de "preguntas"
  Embedding del documento:   [0.3, 0.1, ...]  â†’ Espacio de "respuestas"

  Similitud: 0.65 â†’ No tan alta como deberÃ­a ser
  (porque preguntas y respuestas estÃ¡n en "zonas" diferentes del espacio)

SOLUCIÃ“N: HyDE

  La query del usuario:     "Â¿QuÃ© es LIME?"
  Doc hipotÃ©tico generado:  "LIME es una tÃ©cnica que..." (texto largo)
  El documento REAL dice:    "LIME es una tÃ©cnica que..." (texto largo)

  Embedding del hipotÃ©tico: [0.28, 0.12, ...]  â†’ Espacio de "respuestas"
  Embedding del real:       [0.30, 0.10, ...]  â†’ Espacio de "respuestas"

  Similitud: 0.92 â†’ Â¡Mucho mejor!
  (porque ambos son "respuestas", estÃ¡n en la misma zona)
```

### VisualizaciÃ³n

```text
                    Espacio de Embeddings

    "preguntas"                    "respuestas"
    zone                           zone

    â— query                        â— doc_real
    "Â¿QuÃ© es LIME?"               "LIME es una tÃ©cnica..."

         â•²                        â•±
          â•²  distancia grande    â•±
           â•²                    â•±
            â•²                  â•±
             â•²                â•±
              â•²              â•±

    CON HyDE:
              â— doc_hipotÃ©tico
              "LIME es una tÃ©cnica..."
                    â”‚
                    â”‚ distancia pequeÃ±a
                    â”‚
              â— doc_real
              "LIME es una tÃ©cnica..."
```

---

## Paso 2.6: Triple Strategy Search

Para maximizar la capacidad de recuperaciÃ³n (recall) y cubrir todos los tipos de consultas posibles, el sistema ejecuta **tres estrategias de bÃºsqueda en paralelo**.

### Las 3 Estrategias

1. **HyDE + Hybrid (Conceptual)**:
   - Genera un documento hipotÃ©tico que responde a la pregunta.
   - Busca vectores similares a esa respuesta hipotÃ©tica.
   - Ideal para preguntas complejas o abstractas ("Â¿CÃ³mo funciona la explicabilidad?").

2. **Direct Hybrid (Mixta)**:
   - Busca vectores de la pregunta original.
   - Busca palabras clave exactas (BM25).
   - Ideal para preguntas estÃ¡ndar con terminologÃ­a tÃ©cnica.

3. **Direct Vector (Estructural/Corta)**:
   - Busca vectores directamente en la base de datos sin intermediarios.
   - Ideal para preguntas muy cortas o estructurales ("bibliografÃ­a", "Ã­ndice") donde HyDE puede alucinar y BM25 puede fallar por falta de contexto.

````python
# Pseudo-cÃ³digo de la fusiÃ³n de estrategias

# 1. HyDE
hyde_docs = vector_search(hypothetical_doc)

# 2. Direct Hybrid
keyword_docs = bm25_search(original_query)

# 3. Direct Vector
direct_docs = vector_search(original_query)

# FusiÃ³n
all_candidates = unique(hyde_docs + keyword_docs + direct_docs)

### Â¿Por quÃ© combinar dos estrategias?

```text
Caso 1: VECTOR gana (SemÃ¡ntica)
  Query: "tÃ©cnicas para explicar modelos"
  Documento: "...mÃ©todos interpretativos para cajas negras..."

  â†’ BM25 falla: no hay coincidencia de palabras ("explicar" != "interpretativos").
  â†’ Vector acierta: entiende que significan lo mismo.

Caso 2: BM25 gana (Exactitud)
  Query: "LIME"
  Documento: "La tÃ©cnica LIME se define como..."

  â†’ Vector puede fallar: confunde "LIME" con "lima" (fruta) o conceptos generales.
  â†’ BM25 acierta: encuentra la palabra exacta "LIME".

Caso 3: DIRECT VECTOR gana (Estructural)
  Query: "bibliografÃ­a"
  Documento: "BIBLIOGRAFÃA [1]..."

  â†’ HyDE falla: genera un texto largo sobre quÃ© es una bibliografÃ­a, alejando el vector.
  â†’ BM25 falla: si el chunk es muy heterogÃ©neo (lista de autores), el score de la palabra es bajo.
  â†’ Direct Vector acierta: el embedding de "bibliografÃ­a" estÃ¡ muy cerca del tÃ­tulo "BIBLIOGRAFÃA".
````

### Uso inteligente de queries diferentes

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Este es un truco avanzado que mejora mucho la calidad: â”‚
â”‚                                                         â”‚
â”‚  Vector Search â† usa el doc HIPOTÃ‰TICO (HyDE)           â”‚
â”‚    "LIME es una tÃ©cnica de explicabilidad que..."       â”‚
â”‚    â†’ Busca por significado profundo                     â”‚
â”‚                                                         â”‚
â”‚  BM25 Search â† usa la query ORIGINAL                    â”‚
â”‚    "Â¿QuÃ© es LIME?"                                      â”‚
â”‚    â†’ Busca la palabra "LIME" exacta                     â”‚
â”‚                                                         â”‚
â”‚  Cada retriever recibe la query Ã“PTIMA para su tipo     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Paso 2.7: Reranking (reranker.py)

```python
# Â¿QuÃ© hace?
# Reordena los candidatos por relevancia REAL usando un modelo
# entrenado especÃ­ficamente para determinar relevancia

class Reranker:
    def __init__(self):
        # FlashRank: modelo local, rÃ¡pido, sin API
        self.ranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2")

    def rerank(self, query, documents, top_n=3):
        # Input: 10 candidatos del hybrid search
        # Output: Los 3 mÃ¡s relevantes, ordenados por score
        ...
```

### Â¿Por quÃ© reranking si ya tenemos embeddings?

```text
Los embeddings son RÃPIDOS pero IMPRECISOS:
  Son bi-encoders: codifican query y documento POR SEPARADO
  Luego comparan los vectores con similitud coseno
  â†’ No ven la INTERACCIÃ“N entre query y documento

El reranker es LENTO pero PRECISO:
  Es un cross-encoder: analiza query + documento JUNTOS
  Entiende la relaciÃ³n profunda entre ambos
  â†’ Pero solo puede procesar ~10 docs (no 10,000)

COMBINACIÃ“N Ã“PTIMA:
  1. Embeddings filtran 10,000 â†’ 10 candidatos (rÃ¡pido, recall alto)
  2. Reranker reordena 10 â†’ 3 mejores (lento, precisiÃ³n alta)
```

```text
Ejemplo:

  Query: "Â¿CÃ³mo funciona la explicabilidad local?"

  DespuÃ©s del hybrid search (10 candidatos):
    1. "LIME genera perturbaciones..."        (relevante âœ…)
    2. "El modelo se entrena con Adam..."      (irrelevante âŒ)
    3. "La explicabilidad es un campo..."      (parcial âš ï¸)
    4. "Las predicciones locales permiten..."  (relevante âœ…)
    ...

  DespuÃ©s del reranking (top 3):
    1. "LIME genera perturbaciones..."         score=0.95
    2. "Las predicciones locales permiten..."  score=0.87
    3. "La explicabilidad es un campo..."      score=0.72

  â†’ Los irrelevantes se eliminaron
  â†’ Los relevantes subieron al top
```

---

## Paso 2.8: Parent Document Expansion (parent.py)

```python
# Â¿QuÃ© hace?
# Reemplaza los chunks pequeÃ±os por sus pÃ¡ginas completas

def get_parents_for_chunks(self, chunks, expand_neighbors=True):
    for chunk in chunks:
        page_num = chunk.metadata["page"]  # Este chunk viene de pÃ¡g 5

        pages_to_fetch = [page_num]        # Traer pÃ¡g 5

        if expand_neighbors:
            pages_to_fetch.append(page_num - 1)  # TambiÃ©n pÃ¡g 4
            pages_to_fetch.append(page_num + 1)  # TambiÃ©n pÃ¡g 6

    # Deduplicar: si dos chunks vienen de la misma pÃ¡gina,
    # solo la incluimos una vez
    ...
```

### Â¿Por quÃ© expandir a vecinos?

```text
Caso: Tabla que cruza dos pÃ¡ginas

  PÃ¡gina 5: "Tabla 3. Resultados del modelo:
             | Modelo | Accuracy | Recall |
             | RF     | 0.85     | 0.78   |"

  PÃ¡gina 6: "| SVM    | 0.92     | 0.88   |
             | LIME   | N/A      | N/A    |
             ConclusiÃ³n: SVM obtuvo los mejores..."

  Si el chunk encontrado estÃ¡ en pÃ¡g 5,
  sin expansiÃ³n el LLM solo ve la mitad de la tabla.
  Con expansiÃ³n a pÃ¡g 6, ve la tabla completa.
```

---

## Resumen Visual del Retrieval

```text
         "Â¿QuÃ© es LIME?"
              â”‚
              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Router: SEARCH âœ…  â”‚ â† Guardrail
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Cache: MISS âŒ     â”‚ â† No hay respuesta cacheada
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Contextualizer    â”‚ â† Sin historial, no cambia nada
    â”‚ "Â¿QuÃ© es LIME?"  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ HyDE Generator    â”‚â”€â”€â”€â”€â†’â”‚ "LIME es una tÃ©cnica â”‚
    â”‚                   â”‚     â”‚ de explicabilidad..."â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                           â”‚
             â–¼                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Hybrid Search                                 â”‚
    â”‚                                               â”‚
    â”‚  Hyde Search â†â”€â”€ doc hipotÃ©tico (semÃ¡ntica)   â”‚
    â”‚  BM25 Search   â†â”€â”€ query original (keywords)  â”‚
    â”‚  Direct Vector â†â”€â”€ query original (estructura)â”‚
    â”‚  â†’ X candidatos fusionados                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ FlashRank Reranker                            â”‚
    â”‚ X candidatos â†’ 5 mÃ¡s relevantes               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Parent Expansion                              â”‚
    â”‚ 3 chunks â†’ 5-7 pÃ¡ginas completas              â”‚
    â”‚ (con vecinos para contexto)                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  CONTEXTO LISTO
                  PARA EL LLM
```
